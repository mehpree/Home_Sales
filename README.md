
# Home Sales Data Analysis with SparkSQL
![2D-floor-pla-transparent](https://github.com/mehpree/Home_Sales/assets/131678606/83a7ce03-ac9a-4a41-a7ed-0aae94bf3129)

This project is a comprehensive exploration of home sales data utilizing Apache Spark and SparkSQL. It involves a series of tasks that allow us to derive key insights and metrics from the dataset. Below is a concise overview of the project:

## Project Overview

**Objective**: The primary objective of this project is to analyze home sales data and extract valuable insights using SparkSQL. We will perform various operations, including data ingestion, SQL querying, caching, and runtime analysis, to address specific questions and requirements.

**Key Tasks**:

1.  **Repository Setup**:
    
    -   Create a dedicated GitHub repository named "Home_Sales" for the project.
    -   Clone the repository to your local computer for collaborative work.
    -   Commit and push changes to GitHub regularly to track progress.
2.  **Data Preparation**:
    
    -   Import essential PySpark SQL functions to facilitate data analysis.
    -   Read the provided `home_sales_revised.csv` data into a Spark DataFrame, preparing it for analysis.
3.  **Temporary Table Creation**:
    
    -   Create a temporary table named `home_sales` using the Spark DataFrame. This temporary table will serve as our foundation for SQL queries.
4.  **Data Analysis with SparkSQL**:
    
    -   Address a series of questions using SparkSQL queries, including:
        -   Determining the average price of four-bedroom houses sold each year.
        -   Calculating the average price of homes with specific features (e.g., three bedrooms and three bathrooms) for each year built.
        -   Analyzing homes with specific criteria (e.g., three bedrooms, three bathrooms, two floors, and size ≥ 2,000 square feet) for each year built.
        -   Evaluating the "view" rating for homes priced at or above $350,000 and measuring query runtimes.
5.  **Temporary Table Caching**:
    
    -   Cache the `home_sales` temporary table to enhance query performance for subsequent operations.
6.  **Cache Verification**:
    
    -   Confirm whether the `home_sales` temporary table is successfully cached using PySpark, ensuring data consistency.
7.  **Query Execution on Cached Data**:
    
    -   Re-run the query filtering view ratings with an average price ≥ $350,000 using the cached data. Record query runtime and compare it with the uncached runtime to highlight the advantages of caching.
8.  **Partitioning and Parquet Data Handling**:
    
    -   Implement data partitioning based on the "date_built" field.
    -   Store data in formatted Parquet files, optimizing data storage and retrieval.
9.  **Temporary Table for Parquet Data**:
    
    -   Create a temporary table for the Parquet data, facilitating integration into the analysis.
10.  **Query Execution on Parquet Data**:
    
    -   Execute the query from step 7 on the Parquet temporary table, measuring runtime and comparing it with the uncached runtime.
11.  **Temporary Table Uncaching**:
    
    -   Uncache the `home_sales` temporary table and validate that it is no longer cached using PySpark.
12.  **Notebook Upload**:
    
    -   Conclude the project by downloading the `Home_Sales.ipynb` file and uploading it to the "Home_Sales" GitHub repository, making the analysis accessible to others.

**Conclusion**: This project demonstrates our ability to harness the power of SparkSQL for data analysis, including data manipulation, caching, and runtime optimization. By answering specific questions and effectively managing data, we gain valuable insights into home sales metrics.

The successful completion of this project showcases a strong foundation in data analysis with SparkSQL, setting the stage for further exploration and advanced data analytics projects.

### Reference
Data for this dataset was generated by edX Boot Camps LLC, and is intended for educational purposes only.
